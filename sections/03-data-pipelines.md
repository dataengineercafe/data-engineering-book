# Data Pipelines

## รูปแบบในการสร้าง Data Pipeline

มีรูปแบบ ETL และ ELT ซึ่งทั้ง 2 patterns นี้ใช้กันอย่างกว้างขวางมากที่สุด ความแตกต่างของทั้ง 2 แบบ อยู่ที่ลำดับของตัวอักษร 2 ตัวหลัง (Transform กับ Load)

- Extract (E) การดึงข้อมูลจากแหล่งข้อมูลต่างๆ
- Load (L) การโหลดข้อมูล ไม่ว่าจะเป็นข้อมูลดิบ หรือข้อมูลที่ถูกแปลงแล้ว ไปที่ปลายทาง เช่น Data Warehouse หรือ Data Lake หรือระบบอื่นๆ
- Transform (T) เป็นการแปลงข้อมูลดิบจากแหล่งข้อมูลต่างๆ หรือจากระบบต่างๆ ให้มาอยู่ในรูปแบบที่เราสามารถเอาไปวิเคราะห์ได้ หรือเอาไปทำ Visualization ได้

ขั้นตอน E กับ L อาจจะเรียกรวมๆ ได้เป็น Data Ingestion เพราะว่าดูแล้วมีอะไรที่คล้ายๆ กัน และมีความสามารถเหมือนๆ กันอยู่ แต่อย่างไรก็ดี ตอนที่เราออกแบบ Data Pipeline เราควรที่จะแยกออกจากกันดีกว่า เพราะว่าความซับซ้อนของ E กับ L ต่างกัน

## ความท้าทายในการสร้าง Data Pipeline ที่ดี

ตอนที่เราสร้าง Data Pipeline เรามักจะเจอความท้าทายหลักๆ ประมาณ​ 5 อย่างนี้

1. [Schema เปลี่ยนแปลงอยู่ตลอดเวลา](#1-schema-เปลี่ยนแปลงอยู่ตลอด)
1. [Machine Failure เป็นเรื่องปกติ](#2-machine-failure-เป็นเรื่องปกติ)
1. [การ Scale เพื่อรองรับข้อมูลที่มีขนาดใหญ่ขึ้นเรื่อยๆ](#3-การ-scale-เพื่อรองรับข้อมูลที่มีขนาดใหญ่ขึ้นเรื่อยๆ)
1. [Batch vs. Real-Time](#4-batch-vs-real-time)
1. [การทำ Data Catalog และ Data Lineage](#5-การทำ-data-catalog-และ-data-lineage)

### 1. Schema เปลี่ยนแปลงอยู่ตลอด

แทบจะเป็นปัญหาอับดับ 1 เลยที่ทุกคนต้องเจอ ยิ่งในยุคปัจจุบันที่โลกของซอฟต์แวร์นั้นเปลี่ยนแปลงไปเร็วมาก ธุรกิจเราก็ปรับตัวตามไปด้วย
และแน่นอนว่าจะส่งผลให้ Schema ของข้อมูลนั้นเปลี่ยนไป เราก็ต้องปรับ Data Pipeline ของเราตาม

วิธีการก็มีอยู่หลายวิธีขึ้นอยู่กับสถานการณ์ เช่น ถ้าข้อมูลเราไม่เยอะเท่าไหร่ แล้วการวิเคราะห์ข้อมูลก็ไม่จำเป็นต้องเป็น Real-Time
เราอาจจะ Drop Table ทิ้ง สร้างใหม่ แล้วโหลดข้อมูลตามไป แต่ถ้าข้อมูลเราเยอะมากขึ้น การทำแบบนี้ก็อาจจะเสียเวลาไปเป็นวันๆ
เราก็ต้องหาวิธีอื่นมาแก้ปัญหาให้เหมาะสมกับสถานการณ์

ระบบ Monitoring กับ Logging ดีๆ จะช่วยให้เรารู้ตัวได้ไว และการทำ Schema Version Management ก็สามารถช่วยได้เช่นกัน

### 2. Machine Failure เป็นเรื่องปกติ

งาน Data Pipeline ไม่ได้มีแค่ส่วนโค้ดที่เราต้องดูแล ยังมีเรื่องของ Infrastructure ที่เราใช้อีกด้วย
ระบบหรือตัวเครื่องเซิฟเวอร์ มักจะมีปัญหาประมาณว่า Disk เต็ม ทำให้เขียนไฟล์ไม่ได้บ้าง หรือเครื่องค้างต้องรีสตาร์ท 
รวมไปถึง การที่ระบบ Network หรือ DNS ล่ม แล้วยังต้องมาคอยอัพเกรด Patch อีก (โดยเฉพาะ Security Patch)
ซึ่งสิ่งเหล่านี้เกิดขึ้นเป็นเรื่องปกติ เราหาวิธีรับมือไว้เลยแต่เนิ่นๆ เลย

### 3. การ Scale เพื่อรองรับข้อมูลที่มีขนาดใหญ่ขึ้นเรื่อยๆ

ช่วงแรกๆ ตอนที่มีข้อมูลน้อยๆ Data Pipeline อาจจะใช้เวลาไม่ถึง 10 นาทีก็ทำงานเสร็จแล้ว พอทำเสร็จ เราก็อาจจะไม่ได้เข้ามาดูแลสักเท่าไหร่
แต่นั่งอาจจะเป็นหลุมพลาง (Pitfall) ของหลายๆ คน เนื่องจากข้อมูลจะไหลเข้ามาเรื่อยๆ อยู่ตลอดเวลาอยู่แล้ว
Data Pipeline ของเราก็จะใช้เวลานานขึ้นเรื่อยๆ เช่นกัน แล้วแต่ละองค์กรก็คงไม่ได้มีแค่ Data Pipeline เดียวแน่
ดังนั้นให้นึกถึงการ Scale เอาไว้ด้วยเลย ตั้งแต่เนิ่นๆ ยิ่งองค์กรไหนมีข้อมูลเยอะอยู่แล้ว เรื่องการ Scale เป็นเรื่องที่สำคัญมาก
ตรงนี้จะรวมไปถึงการเลือก Technology ที่เหมาะสมมาใช้ด้วยเช่นกัน

อยากเน้นย้ำเพิ่มเติมว่าให้คิดไว้ตั้งแต่ช่วงแรกๆ เพราะถ้าเรายิ่งปล่อยทิ้งไว้มันก็ยิ่งเหมือน Technical Debt ที่สะสม
ไปเรื่อยๆ จนวันหนึ่งเราจะไม่สามารถแก้มันได้อีกแล้ว เพราะค่าใช้่จายที่จะลงแรงไปปรับปรุงให้ดีขึ้นมันสูงเกินไป

### 4. Batch vs. Real-Time

การเลือกว่าจะเป็นการประมวลผลแบบ Batch หรือ Real-Time ให้ลองทำความเข้าใจธุรกิจ และ Use Case ต่างๆ ก่อน ตรงนี้จะขึ้นอยู่กับบริบทของแต่ละองค์กร
และแต่ละงาน ถึงแม้ว่าตลาดส่วนใหญ่จะเป็นเรื่องการประมวลผลแบบ Batch แต่ก็อยากให้ระลึกไว้เสมอไว้ว่า
งานประเภท Real-Time ก็สามารถสร้างคุณค่าให้กับองค์กรได้เช่นกัน และการสร้าง Data Pipeline แบบ Batch กับแบบ Real-Time
มีการพัฒนาและการดูแลที่แตกต่างกัน ในฐานะที่พวกเราเป็น Data Engineer ควรที่จะศึกษาและทำความเข้าใจไว้ทั้ง 2 แบบ

### 5. การทำ Data Catalog และ Data Lineage

หัวข้อนี้มีความเกี่ยวข้องกับการทำ Data Lake ด้วย ซึ่งหลายคนมักจะมองข้าม เอาไว้ทำทีหลังก็ได้ แล้วสุดท้ายเราก็ลืม หรือไม่ก็เกิดอาการ
Curse of Knowledge ของคนทำข้อมูล ที่ว่ามองแว๊บเดียวก็รู้ว่าอะไรคืออะไร อย่าไปตกหลุมพลางเข้าล่ะ ระลึกไว้เสมอเลยว่าเราไม่ได้ทำงานคนเดียว

ซึ่งถ้าเราอยากให้ทุกคนในองค์กรสามารถใช้ข้อมูลได้อย่างมีประสิทธิภาพ และเอาไปช่วยตัดสินใจในงานของพวกเขาได้ การมี Data Catalog
(เก็บ Metadata ไว้เพื่อให้ค้นหาข้อมูลได้สะดวกและรวดเร็ว) และ Data Lineage (รู้ที่มาที่ไปของข้อมูลว่ามาจากไหน ได้มาได้อย่างไร
โดน Transform มาแบบไหน) ถือว่าเป็นเรื่องที่ขาดไม่ได้เลย
